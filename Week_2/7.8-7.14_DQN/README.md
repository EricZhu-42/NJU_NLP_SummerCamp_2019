# 基于Dueling DQN的SpaceInvaders游戏AI模型

>  时间：2019-07-19   
> 
> 以下部分代码参考了 UCB CS294 HW3 中的课程代码，[代码库链接](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3)

## 项目文件结构
| 名称              | 描述                                              |
| ----------------- | ------------------------------------------------- |
| log\              | 实验过程中输出的日志文件（部分）                  |
| experiments\      | 训练后模型的储存位置                              |
| video\            | 关于模型表现的演示视频                            |
| img\              | 本文档中用到的图片资源                            |
| atari_wrappers.py | 调用 OpenAI gym Atari 环境接口的脚本              |
| logz.py           | 训练过程的记录器脚本                              |
| DQN_utils.py      | DQN依赖脚本，定义了损失函数，学习率变化情况等信息 |
| DQN_learn.py      | DQN学习脚本，定义了具体的学习过程                 |
| DQN_train.py      | DQN训练脚本，模型的训练入口                       |
| DQN_eval.py       | DQN评估脚本，模型的评估入口                       |

## 使用方法

##### 测试已训练好的模型：

1. 运行 `DQN_eval.py` 脚本

2. 等待随机填充经验池（5000step），此时命令行中会输出每轮的Reward（随机决策），无游戏画面显示

3. 经验池填充结束后，开始测试，此时会显示游戏画面，并在命令行中输出每轮的Reward（模型表现）

	> **注1：建议先观看录制的测试视频 (`video\eval.mp4`)，再手动进行测试。**  
	> 注2：在测试前请勿开始新的训练过程，否则可能会覆盖原有的经过训练的模型。

##### 训练模型

1. 删除 `experiments\` 文件夹（若想在原有模型的基础上继续训练，可跳过此步骤）

2. 运行 `DQN_train.py` 脚本

3. 等待随机填充经验池（50000step），此时命令行中无输出，无游戏画面显示。

4. 经验池填充结束后，开始训练。此时会在每训练32个Episode后输出该轮次的各项训练结果，并保存当前模型的各项参数；无游戏画面显示。

   > 注1：因在原模型的基础上继续训练时，设定为不继承原有模型的Exploration Rate，Learning Rate等超参数，因此模型继续训练的初始表现可能不佳。

## 环境说明

> 使用的测试环境如下：
>
> Python == 3.6   
> tensorflow == 1.14.0 (GPU  version)  
> gym ==  0.10.5 **(注意最新版本gym可能无法正常运行)**   
> opencv-python == 4.1.0.25  
> numpy == 1.16.4  
> 
> 游戏环境：SpaceInvaders-v0  
> 
>
> 训练机配置如下：
>
> CPU : I7-8700 (6C12T)   
> GPU: RTX2060 (6G)    
> CUDA = v10.0   
> cuDNN = v7.3.1 

## 问题描述

在SpaceInvaders游戏中，agent需要控制己方飞机摧毁尽可能多的敌方飞行物，从而获得尽可能高的分数。游戏中共有4种合法的操作：不行动 (Noop)，开火 (FIRE)，左移 (Left)，右移 (Right)。

游戏的输出画面为210x160x3的RGB图片，输出值为每轮的游戏得分（Reward）。我们需要通过训练，使得智能体对于当前的游戏状态做出决策，在4种合法操作种选择一种，从而获得更高的游戏分数。

## 方案描述

由于游戏画面的每一帧就是一个状态，我们显然不能构造Q表保存所有状态的 Action-Value 值。为此，我们采用 DQN 解决 SpaceInvaders 游戏的问题。借助神经网络的拟合能力，我们用神经网络来表示 q-function，用神经网络来预测Q值，并通过不断更新神经网络从而学习到最优的行动路径。

首先，由于过于复杂的画面会对模型的学习产生干扰，我们将游戏画面灰度化并二值化，得到210x160的灰度图像，并裁剪调整成84x84的图像（正方形图像便于卷积），将连续4帧的图像堆叠在一起作为神经网络的输入。单独一帧图像无法判断子弹与敌人的移动方向，因此无法为决策提供完整的信息。输入连续4帧的图像在一定程度上赋予了模型基于先前状态进行决策的能力，使其在不使用 RNN 或 LSTM 的情况下也能有较好的表现。

其次，我们对神经网络的结构与超参数进行定义。我们采用基于 CNN 的神经网络，对图像中的特征进行抽取，再通过 Value-Advantage 构造了 Dueling DQN（细节见下节图表）。我们采用相较于平方误差，受离群点的影响程度更小的 [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss) 作为误差函数。在神经网络参数的更改过程中使用滑动平均模型进行优化，提高模型表现。我们对网络的学习率进行分段调整，并采用 Adam 优化器对网络的参数进行优化。

此外，我们将维护一个固定大小的经验池（Replay Buffer），将经历过的状态，采取的行动以及新的状态进行存储。在 q-function 方面，我们同时维护 target-net 与 eval_net 两个网络，分别求出 Q-target 与 Q-eval，并在一定步数后用 eval_net 的参数对 target-net 的参数进行更新。在学习过程中，我们随机抽取经验池中的数据进行学习，打乱经历间的相关性，使得网络的学习过程更有效率。

接着，在模型的决策方面，我们对模型的 Exploration Rate 进行分段调整。在每一个状态下，我们将随机算法产生的结果与当前的 Exploration Rate 进行比较，进而确定下一步动作是随机选择还是使用 q-function 进行决策。训练开始时，将先对经验池进行随机填充。在到达阈值（50000step）后，开始学习过程。每32个 episode 后，将自动生成阶段性训练日志，并对当前的模型进行存储。

本次项目中，我们首先采用了与 DeepMind [论文](https://arxiv.org/abs/1312.5602v1)相同的结构。接着，改进了 q_func 网络的结构，采用了 Dueling DQN 进行测试。最后，保存了一次较为优秀的模型（平均得分约550，略高于论文中得到的结果）。

## q_func 网络结构

| 名称      | 描述                                                         |
| --------- | ------------------------------------------------------------ |
| conv1     | 卷积层，深度为32，卷积核大小为8x8，步长为4，激活函数为ReLU   |
| conv2     | 卷积层，深度为64，卷积核大小为4x4，步长为4，激活函数为ReLU   |
| conv3     | 卷积层，深度为64，卷积核大小为3x3，步长为4，激活函数为ReLU   |
| flatten   | 将卷积的结果变形为一维张量                                   |
| V_fc1     | Value函数的全连接层，以flatten为输入，节点数为512，激活函数为ReLU |
| Value     | Value函数的输出结果，节点数为1                               |
| A_fc1     | Advantage函数的全连接层，以flatten为输入，节点数为512，激活函数为ReLU |
| Advantage | Advantage函数的输出结果，节点数为4（合法动作的数量）         |
| out       | q_func的输出结果，综合考虑了Value和Advantage                 |

## 训练效果

以下选取两次结果（分别为 DeepMind 提出的原始模型与改进后的 Dueling DQN 模型。模型的其他结构与超参数均相同）进行对比说明。

#### DeepMind 原始模型

![pic1](img\1.png) 如图可见，在3500个Episode的训练后，模型的Mean Reward由150提升到超过600.

#### Dueling DQN 模型

![pic2](img\3.png)

如图可见，在3500个Episode的训练后，模型的Mean Reward由150提升到超过500.

#### 模型对比

![pic3](img\4.png)

可以看到，两个模型的表现并无显著不同。一定程度上是由于Dueling DQN模型需要训练的参数个数更多，因而训练的难度更大。 

> 关于训练日志的细节，可以在 `log\` 文件夹中查看。

## 不足与提升

1. 简单的连续4帧堆叠无法真正使模型做出基于时间序列的决策。为此，可以在使用 CNN 对图像进行特征抽取后，加入 LSTM层 (Sequence to Vector) 对序列化的输入进行决策，提升模型的表现。
2. 训练的时间与次数不足，导致未能得出该模型在充分训练状态下的最优表现。应当继续提升训练轮数，获得更好的模型表现。
3. 经验池中不同经历的重要性并不完全相同，随机采样并不是最优的解决方法。因此，应当参考 Prioritized Replay DQN 的思路，使得TD误差较大的经历更容易被采样，提高模型收敛的效率，借此对模型进一步加以改进。
